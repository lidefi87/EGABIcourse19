# Algoritmos de modelos de distribucion de especies

Introducción a varios algoritmos disponibles para modelar distribución de especies.  
  
[Presentation (PDF)](./course_material/C6_Different algorithms.pdf)  
  
## Ejercicios - Práctica con modelos

Para este ejercicio vamos a utilizar un set de datos de superhéroes. De seguro que se sorprenderán cuando comiencen a explorar los datos ambientales.  
  
El objetivo es correr modelos que nos permitan responder dos preguntas:  
  
1) CLASIFICACIÓN: ¿Existe un sesgo hacia la armonización? Es decir, ¿somos capaces de predecir si un personaje es bueno o malo?  
2) REGRESIÓN: ¿Son los hombres de los cómics más fuertes que las mujeres? Es decir, ¿somos capaces de predecir la fuerza total del personaje? Además, ¿es el género uno de los predictores más importantes o hay alguna otra variable que es mejor para predecir la fuerza total?  
  
Aquí haremos lo siguiente:  
  
1) Explorar y limpiar datos  
2) Usar el método de codificación "one-hot" para incluir muchas categorías en el modelo  
3) Construir un árbol de decisión y visualizar resultados. Reconstruir este árbol pero con configuración diferente  
4) Correr un modelo con `gbm.step` y visualizar las figuras de entrenamiento y prueba, así como crear una figura de la importancia de las variables usadas en el modelo  
5) Correr el mismo modelo pero con random forests y contrastar las diferencias en los resultados      
  
***Escoge un problema de clasificación o regresión. La solución para el problema de clasificación está incluida en este documento por si necesitas ayuda. Aquí vamos a correr el modelo GBM solamente para el problema de regresión, pero también puedes probarlo con random forest (RF)***  
  
Empecemos llamando a las librerías que necesitaremos  

```{r sdmalg1, warning=FALSE}
library(tidyverse)
library(dismo)  ## Esta librería incluye el comando gbm.step
library(randomForest)  ## Esta librería incluye randomForest
library(rpart)   ## Esta librería incluye una serie de funciones útiles para los árboles de decisión
library(rpart.plot)
library(gbm)
```
  
### Cargando y limpiando datos

Una vez llamadas las librerías relevantes, vamos a subir el set de datos de superhéroes y los limpiaremos antes de poder iniciar con los modelos.  
  
```{r sdmalg2, echo = FALSE}
## Aqui leemos los datos de superheroes
Supes <- readr::read_csv('docs/course_material/exercises/data/SuperheroDataset.csv')
```

```{r sdmalg3, eval = FALSE}
Supes <- readr::read_csv('SuperheroDataset.csv')
```
  
Para este ejercicios vamos a utilizar las siguientes columnas: Name, Intelligence, Strength, Speed, Durability, Power, Combat, Gender, Race, Creator, Alignment, Total Power. Usamos `tidyverse` para extraer estas columnas.   
  
```{r sdmalg4}
SuperData <- Supes %>%
  dplyr::select(Name, Intelligence:Combat, Alignment:Race, Creator, TotalPower)
```
  
#### Explorando los datos
  
Si exploramos los datos podemos ver algunos elementos interesantes de este set de datos. Usamos `ggplot2` (o `plot` de `R` base si prefieres). 

```{r sdmalg5, warning=FALSE,message=FALSE}

SuperData %>%
  filter(!is.na(Intelligence))%>% 
  ggplot(aes(x=Intelligence,y=Strength)) + geom_point() + geom_smooth(method=lm) + theme_bw()

## Mirando a la linea parece que existe un valor extremadamente bajo de inteligencia.
##   Probablemete puntos atipicos. Veremos a quien le pertenecen estos valores, y que 
##   podria ayudar a determinar si deberiamos incluirlos en nuestro analisis

SuperData %>%
  filter(!is.na(Combat))%>% 
  ggplot(aes(x=Combat,y=Strength)) + geom_point() + geom_smooth(method=lm) + theme_bw()

## Podemos tambien filtrar datos simplemente para explorar el set de datos usando condiciones
## especificas, como por ejemplo:
SuperData %>%
  filter(!is.na(Combat),Gender=='Female')%>% 
  ggplot(aes(x=Combat,y=Strength)) + geom_point() + geom_smooth(method=lm) + theme_bw()


library(GGally)
## Esta es una buena librería para visualizar datos rapidamente en lugar de crear gráficos
## unicos con codigo

quant_df <- SuperData %>% dplyr::select(Intelligence:Gender,TotalPower)
ggpairs(quant_df, progress = FALSE)

```
  
Nota que `total power` está altamente correlacionado con `strength` (fuerza), `speed` (velocidad), `durability` (durabilidad) y `power` (poder). Esto podría influenciar cómo interpretamos el problema de regresión, ya que la correlación de estas variables pueden interferir con la señal de influencia de las variables exploradas. Esto puede interferir en nuestra habilidad de responder nuestras preguntas.  

Si has explorado el set de datos quizás hayas notado lo siguiente:  
  
1)  `Race` (raza) and `Creator` (creador) tienen muchas categorías.  
2)  Existen muchos creadores que solo fueron mencionados una vez, y quizás no sea apropriado incluirlos todos.  
  
```{r sdmalg6}
length(unique(SuperData$Race)) ## Cantidad de categorias unicas en raza
length(unique(SuperData$Creator))  ## Lo mismo pero para creador

## Explremos a Creator:
unique(SuperData$Creator)

SuperData %>% count(Creator)

```

There are many classes here that may not be appropriate to include. For example, creator Ian Fleming (creator of James Bond) only has one character.  This dataset also has Star Trek characters. So, we decide here to filter out the appropriate classes (keeping Marvel Comics, DC Comics, Dark Horse Comics, and Image Comics).  

***Note the usage of %in%***

```{r sdmalg7}
FilteredSupes <- SuperData %>%
  dplyr::filter(Creator %in% c('Marvel Comics','DC Comics','Dark Horse Comics','Image Comics')) %>%
  dplyr::filter(!is.na(Intelligence))
## we do this last part to remove NA data for cleanliness, though trees don't require you to do this!

FilteredSupes

## How many races do we now have??
length(unique(FilteredSupes$Race))

```

Now let's explore this again.

```{r sdmalg8, warning = FALSE, message = FALSE}
quant_df <- FilteredSupes %>% dplyr::select(Intelligence:Gender,TotalPower)
ggpairs(quant_df, progress = FALSE)

```

Not much has really changed, as we have not removed too many rows, which is good thing because it means that those data we removed won't impact the final conclusions in any major way. This just simplifies our data cleaning for this particular exercise. However, we still have lots of 'Race' categories. 

```{r sdmalg9}
FilteredSupes %>% count(Race)
```
Looking at the breakdown of Race, there are many with a count of 1. These can cause some issues when trying to do any cross validation... so we should remove them.

```{r sdmalg10}
FilteredSupes <- FilteredSupes %>% add_count(Race) %>% filter(n > 1)

FilteredSupes %>% count(Race)
```

### One hot encoding!

Race has MANY categories! This COULD cause the trees to get swamped by this predictor.  Also, by one-hot encoding, we can keep data that would otherwise get thrown out if just filtering out categories. For example, if we removed the 'cyborg' category by way of filtering, those character data would be removed as well (they might be useful). One hot encoding turns each category into a binary variable. Can be done easily in tidyverse!

```{r sdmalg11}
OOEdata <- FilteredSupes %>% 
        separate_rows(Race)%>% 
        mutate(count = 1) %>% 
        spread(Race, count, fill = 0, sep = "_")

OOEdata
```
Now, you can very easily explore each category!!   

***Remember though, these need to be converted to factors or else they'll be confused as integers!!***  
 
***magrittr*** package can help do this easily  

```{r sdmalg12, message = FALSE, warning = FALSE}
library(magrittr)

columns <- names(OOEdata)[12:length(names(OOEdata))]
OOEdata <- OOEdata %<>% mutate_at(columns, funs(factor(.)))


quant_df <- OOEdata %>%
  dplyr::select(Intelligence:TotalPower, Race_Human, Race_Cyborg, Race_Mutant)

ggpairs(quant_df, progress = FALSE)

```

This figure is very messy to look at here in R Markdown - but if you eliminate a few of the variables, or use R Studio's zoom button in the plot window, you should be able to explore this!

### The models

#### CART

Let's start by building ourselves a single tree. Feel free to try this with different variables.

##### Classification example
```{r sdmalg13}
## Let's remove 'neutral' characters from our dataset so we can look at if we can predict
##   if they are good or evil. We'll also select just a few columns to work with for
##   simplicity's sake

modeldat <- OOEdata %>% 
  dplyr::filter(Alignment!='neutral') %>%
  dplyr::select(Intelligence:TotalPower, Race_Animal, Race_Human, Race_Demon, Race_God,
                Race_Asgardian, Race_Mutant, Race_Android)

form <- as.formula(paste("Alignment ~ ",paste(names(modeldat),collapse='+')))
form <- update(form, . ~ . -Alignment) 

## Check out help(rpart.control) for a list of the parameters that you can change!
## Try playing with these to get an idea of how they impact a single tree

## To run a regression model, change method to 'anova'
treemod <- rpart(form, method='class',data=modeldat,cp=0.001,maxdepth=6)

rpart.plot(treemod,cex=0.7)

```


#### GBM.step

We're going to 'skip' a step here to a degree and use the gbm.step algorithm, an extension to gbm (generalized boosted regression modelling/ boosted regression trees). gbm.step will calculate the optimal tree to use by way of cross validation, rather than just building a pile of trees and letting the user decide.  

We'll just use the model we had above (classification).  Unfortunately, gbm.step uses column index values, rather than names!

***gbm.step is not particularly well oriented towards multiple class target / dependent variables***
***gbm.step needs to use a data.frame (can't use a tibble), and in the target MUST be a factor***
***gbm.step ALSO needs the TARGET variable to be in the form 0/1 ***

```{r sdmalg14}

modeldat <- modeldat %>% 
  mutate(Alignment_targ = case_when(Alignment == 'good' ~ 0, Alignment == 'bad' ~ 1 )) %>%
  mutate_at(c('Alignment_targ','Gender','Creator'), funs(factor(.)))

modeldf <- data.frame(modeldat)
modeldf$Alignment_targ <- as.numeric(as.character(modeldf$Alignment_targ))

gbmModel <- gbm.step(data=modeldf,gbm.=c(1:6,8:17),gbm.y=18,family="bernoulli",
                     tree.complexity = 10,learning.rate=0.001,bag.fraction=0.8,max.trees = 8000)

```




To get a summary of the output that shows the variable importance rankings:

```{r sdmalg15}
summary(gbmModel)
```

You can also plot the partial dependence plots, and then take it further to explore interactions by plotting the perspective plots

```{r sdmalg16}
gbm.plot(gbmModel,n.plots=5,write.title=F)

## the gbm perspective plot for intelligence and speed versus the fitted values(z)
gbm.perspec(gbmModel,1,3,z.range=c(0.15,0.6),theta=205)

```


#### random forest

Random forest is much more flexible than gbm.step in some respects - for example, you aren't being forced to use column indices! 

This model can be set up much like the CART model from before. 

```{r sdmalg17}
modeldat <- OOEdata %>% 
  dplyr::filter(Alignment!='neutral') %>%
  dplyr::select(Intelligence:TotalPower, Race_Animal, Race_Human, Race_Demon, Race_God,
                Race_Asgardian, Race_Mutant, Race_Android)

## Have to make sure these things are formatted as factors
modeldat <- modeldat %>% 
  mutate(Alignment_targ = case_when(Alignment == 'good' ~ 0, Alignment == 'bad' ~ 1 )) %>%
  mutate_at(c('Alignment_targ','Gender','Creator'), funs(factor(.)))


form <- as.formula(paste("Alignment_targ ~ ",paste(names(modeldat),collapse='+')))
form <- update(form, . ~ . -Alignment) 
form <- update(form, . ~ . -Alignment_targ) 

rf.model <- randomForest(data=data.frame(modeldat),form,mtry=3,maxnodes=10)
importance(rf.model)
partialPlot(rf.model,data.frame(modeldat),Intelligence)
partialPlot(rf.model,data.frame(modeldat),Gender)
```

***The partial dependence plot will focus on the FIRST class. So if you're using 0/1, then 0 will be focused on in the interpretation of the plots. This can be changed with the 'which.class' argument in partialPlot***


### Regression problem

Here, we use gbm.step to run the regression problem. If you want to run it in random forest, give it a try! Ask your instructor for some help, or use Google! There are a lot of resources available. 


```{r sdmalg18}

modeldat <- OOEdata %>% 
  dplyr::filter(Alignment!='neutral') %>%
  dplyr::select(Intelligence:TotalPower, Race_Animal, Race_Human, Race_Demon, Race_God,
                Race_Asgardian, Race_Mutant, Race_Android)

modeldat <- modeldat %>% 
  mutate_at(c('Alignment','Gender','Creator'), funs(factor(.)))

modeldf <- data.frame(modeldat)

## use names(modeldf) to find indices

gbmModel <- gbm.step(data=modeldf,gbm.x=c(1,6:9,11:17),gbm.y=10,family="laplace",
                     tree.complexity = 20,learning.rate=0.001,bag.fraction=0.5,max.trees = 8000)

summary(gbmModel)

gbm.plot(gbmModel,n.plots=5,write.title=F)

## But we are interested in gender, so let's plot that.

gbm.plot(gbmModel,variable.no=4,plot.layout=c(1,1))
```

Interestingly, we see that there does seem to be some sort of bias towards more powerful male characters. However, bearing in mind that the predictive performance (as determined by the summary statistics) is not very high.  

