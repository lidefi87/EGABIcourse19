# Datos ambientales

## Bowerbird/blueant

Con frecuencia, deseamos conocer las condiciones ambientales en nuestros puntos de interés. Para el remoto y vasto Océano Austral, estos datos suelen provenir de fuentes satelitales o modelos. Algunos centros de datos proporcionan herramientas de extracción que seleccionarán un subconjunto de datos según tus necesidades, pero a menudo tiene más sentido almacenar primero colecciones de datos completas localmente y luego trabajar con ellas desde allí.

[bowerbird](https://github.com/ropensci/bowerbird) proporciona un marco para descargar archivos de datos a una colección local y mantenerlos actualizados. El paquete complementario [blueant](https://github.com/AustralianAntarcticDivision/blueant) proporciona un conjunto de definiciones para fuentes de datos del Océano Austral y la Antártida que se pueden utilizar con bowerbird. Esto abarca datos como hielo marino, batimetría y topografía terrestre, oceanografía y reanálisis atmosféricos y pronósticos del clima, de proveedores como NASA, NOAA, Copernicus, NSIDC e Ifremer.

¿Por qué podría ser útil mantener copias locales de conjuntos de datos completos en lugar de solo recuperar subconjuntos de datos según sea necesario?

- Muchos análisis utilizan datos de una variedad de proveedores (en cuyo caso puede que no haya herramientas de extracción dinámicas para todos ellos).
- Los análisis pueden requerir procesar toda una colección de datos para calcular estadísticas apropiadas (por ejemplo, anomalías de temperatura con respecto a una media a largo plazo).
- Diferentes partes del mismo conjunto de datos se utilizan en diferentes análisis, en cuyo caso hacer una copia de todo el conjunto puede ser más fácil de administrar que tener diferentes subconjuntos para diferentes proyectos.
- Una comunidad de investigación local utiliza rutinariamente un conjunto común de datos, en cuyo caso tiene más sentido mantener una copia local para que todos la utilicen en lugar de que diferentes personas descarguen múltiples copias.

En estos casos, mantener copias locales de una variedad de datos de proveedores externos puede ser extremadamente beneficioso, especialmente si esa colección se aloja con una conexión rápida a recursos informáticos locales (máquinas virtuales o instalaciones informáticas de alto rendimiento).


Instala desde GitHub:

```{r ed_install_pkg, eval = FALSE}
remotes::install_github("AustralianAntarcticDivision/blueant")
```

Y carga el paquete antes de usarlo.

```{r ed1, cache = FALSE}
library(blueant)
```

### Paquetes de datos disponibles

Primero, podemos ver los conjuntos de datos disponibles a través de la función sources.

```{r ed2}
srcs <- blueant::sources()
## nombres de las primeras filas
head(srcs$name)
## detalles de la primera fila
srcs[1, ]
```

### Uso

Elige un directorio en el que descargar los datos. Normalmente, esto sería un directorio persistente en tu máquina para que los conjuntos de datos descargados en una sesión permanezcan disponibles para su uso en sesiones posteriores y no necesiten ser descargados nuevamente. Un directorio persistente podría ser algo como c:\data\ (en Windows), o podrías usar el paquete `rappdirs` (la función `user_cache_dir`) para sugerir un directorio adecuado (multiplataforma).

```{r ed0hidden, echo = FALSE, results = "asis", cache = FALSE}
if (grepl("ben_ray", tempdir())) {
  cat("Here we'll use the `c:/data/cache` directory:\n")
  my_data_dir <-  "/data/cache"
} else {
  cat("Here we'll use a temporary directory:\n")
  my_data_dir <-  tempdir()
}
```

```{r ed0a, include = grepl("ben_ray", tempdir()), eval = FALSE, cache = FALSE}
my_data_dir <-  "/data/cache"
```
```{r ed0b, include = !grepl("ben_ray", tempdir()), eval = FALSE, cache = FALSE}
my_data_dir <-  tempdir()
```

Selecciona la fuente de datos que deseamos:

```{r ed4}
data_source <- sources("Southern Ocean marine environmental data")

```

Tener en cuenta que es una buena idea verificar el tamaño del conjunto de datos antes de descargarlo, ¡ya que algunos son bastante grandes! (Aunque si estás ejecutando la descarga de manera interactiva, te preguntará antes de descargar un conjunto de datos grande).

```{r ed5}
data_source$collection_size ## tamaño en GB
```

y descargar los datos:
```{r ed6}
result <- bb_get(data_source, local_file_root = my_data_dir, verbose = TRUE)
```

Ahora tenemos una copia local de nuestros datos. La sincronización se puede ejecutar diariamente para que la colección local esté siempre actualizada; solo descargará archivos nuevos o archivos que hayan cambiado desde la última descarga. Para obtener más información sobre `bowerbird`, consulta el paquete [package vignette](https://ropensci.github.io/bowerbird/articles/bowerbird.html).

El objeto `result` ontiene información sobre los datos que descargamos:

```{r ed7}
result
```

El elemento `result$files` nos proporciona información sobre los archivos:
```{r ed8}
head(result$files[[1]])
```

Estos archivos en particular son netCDF, por lo que se pueden leer, por ejemplo, con los paquetes `raster` o `ncdf4`. Sin embargo, los datos diferentes de diferentes proveedores serán diferentes en términos de cuadrículas, resoluciones, proyecciones, convenciones de nomenclatura de variables y otros aspectos, lo que tiende a complicar estas operaciones. En la siguiente sección, veremos el paquete `raadtools`, que proporciona un conjunto de herramientas para realizar operaciones comunes en estos tipos de datos.


## RAADtools

El paquete [`raadtools`](https://github.com/AustralianAntarcticDivision/raadtools) proporciona una interfaz consistente para una variedad de datos ambientales y similares, y herramientas para trabajar con ellos. Está diseñado para trabajar con colecciones de datos mantenidas por los paquetes`bowerbird`/`blueant` y se basa en el ecosistema de paquetes de R existente para trabajar con datos espaciales, raster y multidimensionales.

Aquí utilizaremos dos conjuntos de datos ambientales diferentes: hielo marino y profundidad del agua. La profundidad del agua no cambia con el tiempo, pero el hielo marino se proporciona con una resolución temporal diaria.

Primero, descargamos datos diarios de hielo marino (solo desde 2013) y el conjunto de datos batimétricos ETOPO2. ETOPO2 está algo desactualizado y tiene una resolución baja en comparación con datos más recientes, pero servirá como un conjunto de datos pequeño para fines de demostración. Esto puede llevar algunos minutos, según la velocidad de tu conexión:

```{r raadt1, eval = FALSE}
src <- bind_rows(
    sources("NSIDC SMMR-SSM/I Nasateam sea ice concentration", hemisphere = "south", time_resolutions = "day",
            years = 2013),
    sources("ETOPO2 bathymetry"))
result <- bb_get(src, local_file_root = my_data_dir, clobber = 0, verbose = TRUE, confirm = NULL)
```

```{r raadt2, echo = FALSE, message = FALSE}
## El código no se muestra en la salida: capture la salida y recórtela un poco.
src <- bind_rows(
    sources("NSIDC SMMR-SSM/I Nasateam sea ice concentration", hemisphere = "south", time_resolutions = "day", years = 2013),
    sources("ETOPO2 bathymetry"))
op <- capture.output(result <- bb_get(src, local_file_root = my_data_dir, clobber = 0, verbose = TRUE, confirm = NULL))
op[4] <- ""
op[5] <- " [... output truncated]"
for (oo in op[1:5]) cat(oo, "\n")
```


Ahora carga el paquete  `raadtools` y dile dónde se ha almacenado nuestra colección de datos::

```{r raadt3, results = "hold", cache = FALSE}
library(raadtools)
set_data_roots(my_data_dir)
```

Digamos que tenemos algunos puntos de interés en el Océano Austral, como una ruta de barco o algunas estaciones donde tomamos muestras marinas, o como usaremos aquí, el rastro de una foca elefante[track of an elephant seal](http://www.meop.net/) mientras se mueve desde las Islas Kerguelen hacia la Antártida y viceversa (Datos de IMOS 2018[^1], proporcionados como parte del paquete `SOmap`).

```{r get_track_data, message = FALSE, warning = FALSE, cache = FALSE}
data("SOmap_data", package = "SOmap")
ele <- SOmap_data$mirounga_leonina %>% dplyr::filter(id == "ct96-05-13")
```

Define nuestra región espacial de interés y extrae los datos batimétricos de esta región, utilizando los archivos ETOPO2 que acabamos de descargar:

```{r raadt4}
roi <- round(c(range(ele$lon), range(ele$lat)) + c(-2, 2, -2, 2))
bx <- readtopo("etopo2", xylim = roi)
```

Y ahora podemos hacer un gráfico simple de nuestra ruta superpuesta en la batimetría:

```{r raadt5}
plot(bx)
lines(ele$lon, ele$lat)
```

El verdadero poder de `raadtools` proviene de sus funciones de extracción. Podemos extraer los valores de profundidad a lo largo de nuestra ruta utilizando la función `raadtools::extract()`. Le pasamos la función lectora de datos a utilizar (readtopo), los datos a los que aplicarla `(ele[, c("lon", "lat")])` y cualquier otra opción para pasar a la función lectora (en este caso, especificando la fuente de datos topográficos `topo = "etopo2"`):

```{r raadt6}
ele$depth <- raadtools::extract(readtopo, ele[, c("lon", "lat")], topo = "etopo2")
```

Grafica el histograma de los valores de profundidad, mostrando que la mayoría de los puntos de la ruta se encuentran en aguas relativamente poco profundas:

```{r raadt7}
with(ele, hist(depth, breaks = 20))
```

Este tipo de extracción también funcionará con datos que varían con el tiempo, por ejemplo, podemos extraer las condiciones de hielo marino a lo largo de nuestra ruta, según la ubicación y el tiempo de cada punto de la ruta:

```{r raadt8, results = "none", message = FALSE, warning = FALSE}
ele$ice <- raadtools::extract(readice, ele[, c("lon", "lat", "date")])
```
```{r raadt9}
## A los puntos fuera de la cuadrícula de hielo les faltarán valores de hielo, así que rellénelos con ceros.
ele$ice[is.na(ele$ice)] <- 0
with(ele, plot(date, ice, type = "l"))
```

## Other useful packages

- El proyecto [PolarWatch](https://polarwatch.noaa.gov/) tiene como objetivo facilitar el descubrimiento de datos y el uso más amplio de conjuntos de datos de teledetección del océano de altas latitudes. El servidor ERDDAP dedicado (https://polarwatch.noaa.gov/erddap) es accesible para usuarios de R con [rerddap](https://cran.r-project.org/package=rerddap).

- [rsoi](https://cran.r-project.org/package=rsoi) descarga los datos más actualizados del Índice de Oscilación del Sur, el Índice de Niño Oceánico y la Oscilación del Giro del Pacífico Norte.

- Los datos de reflectancia satelital son una base común para estimar clorofila-a y otros parámetros de fitoplancton a escala de cuencas oceánicas. Los productos globales están ampliamente disponibles; sin embargo, es probable que los algoritmos específicos del Océano Austral proporcionen mejores estimaciones en estas regiones. croc implementa el algoritmo del Océano Austral de Johnson et al. (2013).

- Mas ampliamente, [oce](https://cran.r-project.org/package=oce) proporciona una amplia gama de herramientas para leer, procesar y mostrar datos oceanográficos, incluyendo mediciones de boyas Argo y CTD, datos seccionales, series temporales del nivel del mar y datos costeros y topográficos.

- [fda.oce](https://github.com/EPauthenet/fda.oce) proporciona análisis de datos funcionales de perfiles oceanográficos para la detección de frentes, identificación de masas de agua, clasificación no supervisada o supervisada, comparación de modelos, calibración de datos y más.

- [distancetocoast](https://github.com/mdsumner/distancetocoast) proporciona datos de "distancia a la costa" para coordenadas de longitud y latitud.

- [geodist](https://github.com/hypertidy/geodist) para el cálculo muy rápido de distancias geodésicas.
